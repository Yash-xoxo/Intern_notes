AI ML â€“ Day 4 Lecture Notes (Cards Format)
ðŸ“Œ Card 1: Machine Learning Overview
ML aims to learn from past data (experience) to make predictions.

Just like humans learn from hit and trial, ML models improve via iterations.

ðŸ“Œ Card 2: Supervised Learning Structure
Supervised Learning = Input (X) + Output (Y)

Objective: Learn a function Y = f(X) from data

Example: Hours studied (X) â†’ Marks scored (Y)

ðŸ“Œ Card 3: Linear Regression Concept
A basic ML algorithm that assumes a linear relationship between X and Y.

Mathematical form:

ini
Copy
Edit
Y = W * X + b
W = Weight (how much X affects Y)

b = Bias (fixed constant)

ðŸ“Œ Card 4: What is a Model?
A model is a mathematical function that maps inputs to outputs.

In ML, a model learns W and b by minimizing error (loss) between actual Y and predicted Y.

ðŸ“Œ Card 5: Understanding "Loss"
Loss = Actual Y â€“ Predicted Y

The goal of ML: Minimize loss

Common loss functions:

Mean Squared Error (MSE)

Mean Absolute Error (MAE)

ðŸ“Œ Card 6: Gradient Descent
Technique used to find the best W and b.

Works by taking steps in the direction where loss decreases fastest.

It updates W as:

ini
Copy
Edit
W = W - Î± * (dLoss/dW)
Î± = learning rate

ðŸ“Œ Card 7: Hit & Trial to Learning
Early ML models used manual trial and error to tune weights.

Modern ML automates this using algorithms like gradient descent.

ðŸ“Œ Card 8: Role of Data
The quality of learning depends heavily on:

Relevant features (X)

Clean, labeled data

More data â†’ better generalization

ðŸ“Œ Card 9: Overfitting vs Underfitting
Overfitting: Model memorizes training data, performs poorly on new data.

Underfitting: Model is too simple to learn the patterns.

ðŸ“Œ Card 10: Generalization
Generalization = Model's ability to perform well on unseen data

Achieved by:

Training on diverse datasets

Avoiding noise and irrelevant features

ðŸ“Œ Card 11: Training & Testing
Split dataset into:

Training Set: To teach the model

Testing Set: To evaluate performance

Optional: Use Validation Set for tuning

ðŸ“Œ Card 12: Model Evaluation Metrics
For Regression:

MSE, RMSE, MAE, RÂ²

For Classification:

Accuracy, Precision, Recall, F1 Score

ðŸ“Œ Card 13: Practical Cycle of ML
Define Problem Statement

Collect Data

Preprocess Data (clean, transform)

Split into train/test

Choose Algorithm

Train Model

Evaluate Model

Tune Hyperparameters

Deploy Model

Monitor & Retrain

ðŸ“Œ Card 14: Obvious Truth
Machine Learning is just automated experience-based prediction.
You feed data â†’ Model learns patterns â†’ Makes future predictions.

  Card 1: Best Fit Line in Linear Regression
A best fit line is the straight line that best represents data points in a scatter plot.

It's determined by minimizing the loss (error) between actual and predicted values.

The slope (W) of the line defines its angle or steepness.

Goal: Find the W that makes the line pass as close as possible to all points.

ðŸ“Œ Card 2: Slope (W) and Prediction
W = slope = angle of the line

When W = correct value, predicted values (YÌ‚) are accurate â†’ loss = 0

If W is too high or low, the model predicts wrongly and loss increases

This is corrected using gradient descent

ðŸ“Œ Card 3: Loss Function Recap
Formula:

ini
Copy
Edit
Loss = Actual Y â€“ Predicted YÌ‚
If prediction is perfect, Y - YÌ‚ = 0, i.e., loss = 0

Loss function helps us learn the correct W by minimizing error

ðŸ“Œ Card 4: Role of Predicted vs Actual
Y = Actual value (what really happened)

YÌ‚ = Predicted value (model's output)

The difference between them defines model accuracy

This comparison helps update W in each training step

ðŸ“Œ Card 5: Graphical Understanding
If your prediction is consistently off, your line is wrong

Youâ€™ll need to adjust W until your predictions align with actual values

Final goal: a line where all points lie close â†’ best fit

ðŸ“Œ Card 6: Why Real-time Models are Hard
Real-time ML training is rarely used because:

Data keeps changing

Requires constant retraining

Expensive and inefficient

Example: ChatGPT is trained months earlier and then fine-tuned periodically

ðŸ“Œ Card 7: Fine-tuning Concept
ML models are pre-trained on historical data

Later, they are fine-tuned with new data when required

This is faster than retraining from scratch

ðŸ“Œ Card 8: Real-time Data vs Historical
Historical data is:

Cleaned

Labeled

Structured

Real-time data is:

Noisy

Incomplete

Unpredictable

Hence, most models donâ€™t train in real-time, they predict in real-time

ðŸ“Œ Card 9: Visualization Limits in ML
Most visual ML models are plotted in 2D or 3D

Higher-dimensional data (like 4D, 5D) is difficult to visualize

These are called hyperspaces or feature spaces

Human mind cannot visualize >3D, so we rely on mathematics

ðŸ“Œ Card 10: Time as a Dimension
Time is also a dimension in ML, e.g., in time-series data

A data point like "marks over time" creates a 4D graph:

X: study hours

Y: marks

Z: student ID

T: time

ðŸ“Œ Card 11: Extra Dimensions in ML
Concepts like astral planes, quantum states or spiritual dimensions were briefly touched.

The key idea is: ML handles multi-dimensional data, even when we can't visualize it.

ðŸ“Œ Card 12: Linearity vs Complexity
Not all datasets fit a straight line.

Some data requires non-linear models (curves, complex relations)

Thatâ€™s where polynomial regression, decision trees, or neural nets come in.

