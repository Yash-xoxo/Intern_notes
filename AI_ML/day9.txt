AI/ML – Day 9 Notes
🔁 1. Train Once, Use Many Times
You can train a model once, save it, and load it later for future predictions.

This avoids retraining the model every time.

Tools like Joblib or Pickle help save and reload models.

📊 2. Recap of Linear Regression
We predict Y (Salary) based on X (Experience).

Use:

python
Copy code
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
📈 3. Feature Engineering via Visualization
Use matplotlib.pyplot to plot scatter graphs:

python
Copy code
import matplotlib.pyplot as plt
plt.scatter(X, y)
Visualizing data helps:

Understand correlation

Identify trends (e.g., more experience = more salary)

🧠 4. Brain Processing During Graph Reading
The brain observes:

"Higher experience → higher salary"

Patterns in point distribution

This intuition helps design better models.

🧪 5. Discovering Useful Features
Example:

Experience is a good feature for predicting salary.

Name or luck is not a useful feature.

Eliminate features with zero weight (contribute nothing).

📘 6. Weight (W) and Bias (b)
Model:

ini
Copy code
Y = W * X + b
W: how much a feature affects target

b: prediction when X = 0

Weight close to 0 → feature not important

🧠 7. Field Work & Feature Discovery
In real-world ML, finding the right features (X) requires domain knowledge.

Example: success may depend on hours worked, not name or age.

📉 8. Eliminating Weak Features
If a feature doesn’t affect the target (Y), its weight becomes 0.

This means:

Feature has no predictive power

Should be eliminated during training

🔁 9. Learning from Scatter Plot
Best-fit line indicates:

As experience ↑, salary ↑

The relation is positive & linear

Model learns this pattern by minimizing error across all points

📉 10. Real-World Implication for Companies
Companies use graphs to:

Judge performance

Decide hiring/promotions

If more experience ≠ better output, company may shift hiring criteria

🧪 11. Prediction ≠ 100% Accuracy
Prediction line (model) tries to approximate reality, not match it exactly.

Real Value = Y
Predicted Value = Ŷ (Y-hat)
Error = Y - Ŷ

🔄 12. Residual or Loss
Loss = difference between actual and predicted values.

Even with best-fit line, error ≠ 0.

Goal: minimize, not eliminate error.

📍 13. Example: Calculating Prediction and Loss
Given:

Actual salary = ₹39,000

Predicted by model = ₹36,000

Loss = 39,000 - 36,000 = ₹3,000

Accuracy = 36,000 / 39,000 ≈ 92%

🪜 14. How Line is Fitted
Start with random W and b

Model checks:

“How bad is prediction?”

Updates W and b via hit & trial to reduce loss

This process is known as Gradient Descent

📊 15. Best-Fit Line = Minimum Error Line
Line that minimizes total error over all points.

Cannot guarantee 100% accuracy.

It passes closest to all points in average sense.

🧮 16. Loss Calculation (for all data)
For each point:

ini
Copy code
Loss = |Y - Ŷ|  → absolute value
Total Loss:

css
Copy code
Sum of all individual losses / total records
This gives MAE: Mean Absolute Error

📌 17. MAE – Mean Absolute Error
MAE = average of all errors (ignores negative/positive)

No direction, just distance from truth

Helps evaluate how "off" your model is on average

🧠 18. Intuition: Jaipur to Mumbai
True answer = Mumbai
Predicted = somewhere else
Loss = “how far from Mumbai?”

Model tries to reach as close to ground truth (real Y) as possible

🧠 19. Old vs New ML Approaches
Traditional ML: Linear Regression using Scikit-learn

Modern ML: Neural Networks, Deep Learning

Same dataset → different approaches → same goal

🧪 20. Which Model is Better?
Compare based on:

Lower loss

Higher accuracy

A model that gives lower error is preferred

🧪 21. Learning Phase of Machine
Just like humans take exams to test learning

ML models test via error evaluation

Less error → better learning

🔄 22. Final Concept Recap
Goal of ML: Find W and b that minimize prediction loss

Learning = Trying values → checking error → improving

Prediction = Ŷ = W * X + b

Real-world = Error ≠ 0

Best model = one with lowest loss

Let me know if you’d like:

These notes as a downloadable .md or .pdf

A visual diagram (workflow or formula map)

Practice assignments for Day 9

I’m ready when you are for Day 10 or related coding tasks!



🧠 AI/ML Day 9 – Part 2 Notes
📌 1. Loss Minimization and Optimization
Goal of ML: Minimize loss (difference between predicted and actual values).

Loss is used to evaluate if weight W and bias b are good.

You don’t always reach 0 loss — the goal is to get as close as possible.

📌 2. Loss Optimization = Global Goal
"Optimize" in ML means reducing error and improving accuracy.

Example: reducing fuel loss in a vehicle = optimization.

Similarly, ML models optimize performance by reducing loss.

📌 3. Best Fit Line and Loss Visualization
The model tries to draw a best-fit line that passes close to all real data points.

The distance between line (prediction) and points (actual) is loss.

Formula: MAE (Mean Absolute Error) is used to measure average loss.

📌 4. W and Loss Graph (Convex Curve)
If W changes, loss also changes.

At some value of W, loss is lowest = optimal weight.

Graph looks like a U-shape — the bottom point = minimum loss.

📌 5. Local Minimum Concept
From Calculus/Derivatives, the point where loss is lowest is called Local Minimum.

ML models aim to find this local minimum using optimization techniques like gradient descent.

📌 6. Global Minimum vs Local Minimum
Global Minimum = lowest loss possible overall.

Some models might settle in a local minimum, but may not be globally best.

This distinction becomes important in deep learning.

📌 7. Analogy: Reaching Mumbai (Optimization Journey)
Reaching Mumbai = reaching minimum loss.

Steps you take = how fast the model learns.

Big steps → faster arrival, but you may overshoot.

Small steps → slow, but precise.

📌 8. Learning Rate (LR)
Learning Rate = how big a step the model takes while learning.

Large LR = faster training but risk of overshooting minimum.

Small LR = slow training but more precise.

📌 9. Balancing Learning Rate
Learning rate must be chosen carefully:

Too high → you skip over the best point.

Too low → takes forever to train.

Ideal LR gives a balance between speed and accuracy.

📌 10. CPU, Time, and Cost Optimization
Faster training (via better LR) saves:

CPU cycles

Time

Money

Example: OpenAI vs other startups — some models train faster with less cost due to smart optimization.

📌 11. Trade-off: Accuracy vs Speed
Faster training may reduce accuracy slightly.

More accurate models may take more time and resources.

Choose based on project need: accuracy vs speed vs cost.

📌 12. MAE – Mean Absolute Error
MAE measures average absolute difference between actual and predicted values.

Example:

python
Copy code
MAE = sum(abs(Y - Ŷ)) / n
Lower MAE = better model.

📌 13. MSE and RMSE
MSE (Mean Squared Error): squares the differences → more penalty to large errors.

python
Copy code
MSE = sum((Y - Ŷ)^2) / n
RMSE (Root Mean Squared Error): square root of MSE, brings it to original units.

📌 14. Calculating MAE in Code
python
Copy code
from sklearn.metrics import mean_absolute_error

y_true = [39000, 46000, 37000]
y_pred = [36000, 38000, 39000]

mae = mean_absolute_error(y_true, y_pred)
print(mae)
📌 15. Error vs Cost
Loss (error) for one data point = difference between actual and predicted.

Cost = average loss over all data points.

Cost Function = total indicator of model performance.

📌 16. Cost Function Definition
Cost function measures how far the model is from the ground truth.

Used to compare models:

Lower cost = better model

Often discussed in research papers and model benchmarks.

📌 17. Error Evaluation with Cost
If model A has cost = 10 and model B has cost = 5 → model B is better.

Comparing cost across models helps choose the right algorithm.

📌 18. Real Example of MAE
For a 3-record dataset:

Record 1: Predicted 36K, Actual 39K → Error = 3K

Record 2: Predicted 38K, Actual 46K → Error = 8K

Record 3: Predicted 39K, Actual 37K → Error = 2K

MAE = (3K + 8K + 2K) / 3 = 4.3K

📌 19. Conclusion Summary
MAE, MSE, RMSE = tools to evaluate model error

Lower error → better model

Use sklearn.metrics to compute them

Training a model = finding W and b that minimize cost

Cost Function = tells how well the model performs
