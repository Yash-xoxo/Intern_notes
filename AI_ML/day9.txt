AI/ML â€“ Day 9 Notes
ğŸ” 1. Train Once, Use Many Times
You can train a model once, save it, and load it later for future predictions.

This avoids retraining the model every time.

Tools like Joblib or Pickle help save and reload models.

ğŸ“Š 2. Recap of Linear Regression
We predict Y (Salary) based on X (Experience).

Use:

python
Copy code
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
ğŸ“ˆ 3. Feature Engineering via Visualization
Use matplotlib.pyplot to plot scatter graphs:

python
Copy code
import matplotlib.pyplot as plt
plt.scatter(X, y)
Visualizing data helps:

Understand correlation

Identify trends (e.g., more experience = more salary)

ğŸ§  4. Brain Processing During Graph Reading
The brain observes:

"Higher experience â†’ higher salary"

Patterns in point distribution

This intuition helps design better models.

ğŸ§ª 5. Discovering Useful Features
Example:

Experience is a good feature for predicting salary.

Name or luck is not a useful feature.

Eliminate features with zero weight (contribute nothing).

ğŸ“˜ 6. Weight (W) and Bias (b)
Model:

ini
Copy code
Y = W * X + b
W: how much a feature affects target

b: prediction when X = 0

Weight close to 0 â†’ feature not important

ğŸ§  7. Field Work & Feature Discovery
In real-world ML, finding the right features (X) requires domain knowledge.

Example: success may depend on hours worked, not name or age.

ğŸ“‰ 8. Eliminating Weak Features
If a feature doesnâ€™t affect the target (Y), its weight becomes 0.

This means:

Feature has no predictive power

Should be eliminated during training

ğŸ” 9. Learning from Scatter Plot
Best-fit line indicates:

As experience â†‘, salary â†‘

The relation is positive & linear

Model learns this pattern by minimizing error across all points

ğŸ“‰ 10. Real-World Implication for Companies
Companies use graphs to:

Judge performance

Decide hiring/promotions

If more experience â‰  better output, company may shift hiring criteria

ğŸ§ª 11. Prediction â‰  100% Accuracy
Prediction line (model) tries to approximate reality, not match it exactly.

Real Value = Y
Predicted Value = YÌ‚ (Y-hat)
Error = Y - YÌ‚

ğŸ”„ 12. Residual or Loss
Loss = difference between actual and predicted values.

Even with best-fit line, error â‰  0.

Goal: minimize, not eliminate error.

ğŸ“ 13. Example: Calculating Prediction and Loss
Given:

Actual salary = â‚¹39,000

Predicted by model = â‚¹36,000

Loss = 39,000 - 36,000 = â‚¹3,000

Accuracy = 36,000 / 39,000 â‰ˆ 92%

ğŸªœ 14. How Line is Fitted
Start with random W and b

Model checks:

â€œHow bad is prediction?â€

Updates W and b via hit & trial to reduce loss

This process is known as Gradient Descent

ğŸ“Š 15. Best-Fit Line = Minimum Error Line
Line that minimizes total error over all points.

Cannot guarantee 100% accuracy.

It passes closest to all points in average sense.

ğŸ§® 16. Loss Calculation (for all data)
For each point:

ini
Copy code
Loss = |Y - YÌ‚|  â†’ absolute value
Total Loss:

css
Copy code
Sum of all individual losses / total records
This gives MAE: Mean Absolute Error

ğŸ“Œ 17. MAE â€“ Mean Absolute Error
MAE = average of all errors (ignores negative/positive)

No direction, just distance from truth

Helps evaluate how "off" your model is on average

ğŸ§  18. Intuition: Jaipur to Mumbai
True answer = Mumbai
Predicted = somewhere else
Loss = â€œhow far from Mumbai?â€

Model tries to reach as close to ground truth (real Y) as possible

ğŸ§  19. Old vs New ML Approaches
Traditional ML: Linear Regression using Scikit-learn

Modern ML: Neural Networks, Deep Learning

Same dataset â†’ different approaches â†’ same goal

ğŸ§ª 20. Which Model is Better?
Compare based on:

Lower loss

Higher accuracy

A model that gives lower error is preferred

ğŸ§ª 21. Learning Phase of Machine
Just like humans take exams to test learning

ML models test via error evaluation

Less error â†’ better learning

ğŸ”„ 22. Final Concept Recap
Goal of ML: Find W and b that minimize prediction loss

Learning = Trying values â†’ checking error â†’ improving

Prediction = YÌ‚ = W * X + b

Real-world = Error â‰  0

Best model = one with lowest loss

Let me know if youâ€™d like:

These notes as a downloadable .md or .pdf

A visual diagram (workflow or formula map)

Practice assignments for Day 9

Iâ€™m ready when you are for Day 10 or related coding tasks!



ğŸ§  AI/ML Day 9 â€“ Part 2 Notes
ğŸ“Œ 1. Loss Minimization and Optimization
Goal of ML: Minimize loss (difference between predicted and actual values).

Loss is used to evaluate if weight W and bias b are good.

You donâ€™t always reach 0 loss â€” the goal is to get as close as possible.

ğŸ“Œ 2. Loss Optimization = Global Goal
"Optimize" in ML means reducing error and improving accuracy.

Example: reducing fuel loss in a vehicle = optimization.

Similarly, ML models optimize performance by reducing loss.

ğŸ“Œ 3. Best Fit Line and Loss Visualization
The model tries to draw a best-fit line that passes close to all real data points.

The distance between line (prediction) and points (actual) is loss.

Formula: MAE (Mean Absolute Error) is used to measure average loss.

ğŸ“Œ 4. W and Loss Graph (Convex Curve)
If W changes, loss also changes.

At some value of W, loss is lowest = optimal weight.

Graph looks like a U-shape â€” the bottom point = minimum loss.

ğŸ“Œ 5. Local Minimum Concept
From Calculus/Derivatives, the point where loss is lowest is called Local Minimum.

ML models aim to find this local minimum using optimization techniques like gradient descent.

ğŸ“Œ 6. Global Minimum vs Local Minimum
Global Minimum = lowest loss possible overall.

Some models might settle in a local minimum, but may not be globally best.

This distinction becomes important in deep learning.

ğŸ“Œ 7. Analogy: Reaching Mumbai (Optimization Journey)
Reaching Mumbai = reaching minimum loss.

Steps you take = how fast the model learns.

Big steps â†’ faster arrival, but you may overshoot.

Small steps â†’ slow, but precise.

ğŸ“Œ 8. Learning Rate (LR)
Learning Rate = how big a step the model takes while learning.

Large LR = faster training but risk of overshooting minimum.

Small LR = slow training but more precise.

ğŸ“Œ 9. Balancing Learning Rate
Learning rate must be chosen carefully:

Too high â†’ you skip over the best point.

Too low â†’ takes forever to train.

Ideal LR gives a balance between speed and accuracy.

ğŸ“Œ 10. CPU, Time, and Cost Optimization
Faster training (via better LR) saves:

CPU cycles

Time

Money

Example: OpenAI vs other startups â€” some models train faster with less cost due to smart optimization.

ğŸ“Œ 11. Trade-off: Accuracy vs Speed
Faster training may reduce accuracy slightly.

More accurate models may take more time and resources.

Choose based on project need: accuracy vs speed vs cost.

ğŸ“Œ 12. MAE â€“ Mean Absolute Error
MAE measures average absolute difference between actual and predicted values.

Example:

python
Copy code
MAE = sum(abs(Y - YÌ‚)) / n
Lower MAE = better model.

ğŸ“Œ 13. MSE and RMSE
MSE (Mean Squared Error): squares the differences â†’ more penalty to large errors.

python
Copy code
MSE = sum((Y - YÌ‚)^2) / n
RMSE (Root Mean Squared Error): square root of MSE, brings it to original units.

ğŸ“Œ 14. Calculating MAE in Code
python
Copy code
from sklearn.metrics import mean_absolute_error

y_true = [39000, 46000, 37000]
y_pred = [36000, 38000, 39000]

mae = mean_absolute_error(y_true, y_pred)
print(mae)
ğŸ“Œ 15. Error vs Cost
Loss (error) for one data point = difference between actual and predicted.

Cost = average loss over all data points.

Cost Function = total indicator of model performance.

ğŸ“Œ 16. Cost Function Definition
Cost function measures how far the model is from the ground truth.

Used to compare models:

Lower cost = better model

Often discussed in research papers and model benchmarks.

ğŸ“Œ 17. Error Evaluation with Cost
If model A has cost = 10 and model B has cost = 5 â†’ model B is better.

Comparing cost across models helps choose the right algorithm.

ğŸ“Œ 18. Real Example of MAE
For a 3-record dataset:

Record 1: Predicted 36K, Actual 39K â†’ Error = 3K

Record 2: Predicted 38K, Actual 46K â†’ Error = 8K

Record 3: Predicted 39K, Actual 37K â†’ Error = 2K

MAE = (3K + 8K + 2K) / 3 = 4.3K

ğŸ“Œ 19. Conclusion Summary
MAE, MSE, RMSE = tools to evaluate model error

Lower error â†’ better model

Use sklearn.metrics to compute them

Training a model = finding W and b that minimize cost

Cost Function = tells how well the model performs
